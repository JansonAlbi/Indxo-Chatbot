{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d95d743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://indxo.ai/page-sitemap.xml...\n",
      "Saved content from https://indxo.ai/page-sitemap.xml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to get URLs from the sitemap\n",
    "def get_urls_from_sitemap(sitemap_url):\n",
    "    response = requests.get(sitemap_url)\n",
    "    # Use 'lxml' for XML parsing\n",
    "    soup = BeautifulSoup(response.content, 'lxml')  \n",
    "    urls = [loc.text for loc in soup.find_all('loc')]  # Extract all URLs\n",
    "    return urls\n",
    "\n",
    "# Function to scrape content from a single URL\n",
    "def scrape_content_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')  # Use 'lxml' for HTML parsing\n",
    "    \n",
    "    # Get all text from the page, adjust based on the structure of the page\n",
    "    content = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Function to save content into a .txt file\n",
    "def save_content_to_txt(filename, content):\n",
    "    with open(filename, 'a', encoding='utf-8') as file:\n",
    "        file.write(content + '\\n\\n')  # Separate each page content with two newlines\n",
    "\n",
    "# Main function to scrape all URLs and save content\n",
    "def scrape_and_save(sitemap_url, output_filename):\n",
    "    urls = get_urls_from_sitemap(sitemap_url)\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"Scraping {url}...\")\n",
    "        content = scrape_content_from_url(url)\n",
    "        save_content_to_txt(output_filename, content)\n",
    "        print(f\"Saved content from {url}\\n\")\n",
    "\n",
    "# URL of the sitemap\n",
    "sitemap_url = 'https://indxo.ai/sitemap_index.xml'\n",
    "\n",
    "# Output filename\n",
    "output_filename = 'scraped_content.txt'\n",
    "\n",
    "# Start scraping and saving\n",
    "scrape_and_save(sitemap_url, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81b15d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
